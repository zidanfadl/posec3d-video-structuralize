{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7df9318-3492-43a9-9f19-5a589a6e57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f232e4e6-c864-4e1b-b731-5572c316b614",
   "metadata": {},
   "source": [
    "## Testing - Single Recognition [E'19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db1f70ed-737b-4612-9591-3d99766ff2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[                                                  ] 0/122, elapsed: 0s, ETA:10/06 01:54:37 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "10/06 01:54:37 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 122/122, 5.2 task/s, elapsed: 24s, ETA:     0s\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 122/122, 16.5 task/s, elapsed: 7s, ETA:     0s\n",
      "Loads checkpoint by local backend from path: work_dirs/ciis_2019_best-140/best_acc_top1_epoch_140.pth\n",
      "Drawing skeleton for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 122/122, 585.3 task/s, elapsed: 0s, ETA:     0s\n",
      "Moviepy - Building video data/test_video/e19/frontal-statis_satu-objek_aksi-tetap_out.mp4.\n",
      "Moviepy - Writing video data/test_video/e19/frontal-statis_satu-objek_aksi-tetap_out.mp4\n",
      "\n",
      "Moviepy - Done !                                                                \n",
      "Moviepy - video ready data/test_video/e19/frontal-statis_satu-objek_aksi-tetap_out.mp4\n",
      "Moviepy - Building video data/test_video/e19/frontal-statis_satu-objek_aksi-tetap_out.mp4.\n",
      "Moviepy - Writing video data/test_video/e19/frontal-statis_satu-objek_aksi-tetap_out.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready data/test_video/e19/frontal-statis_satu-objek_aksi-tetap_out.mp4\n"
     ]
    }
   ],
   "source": [
    "video = \"data/test_video/e19/frontal-statis_satu-objek_aksi-tetap.mp4\"\n",
    "out_filename = \"data/test_video/e19/frontal-statis_satu-objek_aksi-tetap_out.mp4\"\n",
    "\n",
    "!python mmaction2/demo/demo_skeleton.py {video} {out_filename} \\\n",
    "    --det-config mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py \\\n",
    "    --det-checkpoint http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth \\\n",
    "    --det-score-thr 0.9 \\\n",
    "\\\n",
    "    --pose-config mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py \\\n",
    "    --pose-checkpoint https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth \\\n",
    "\\\n",
    "    --config configs/skeleton/posec3d/ciis_2019.py \\\n",
    "    --checkpoint work_dirs/ciis_2019_best-140/best_acc_top1_epoch_140.pth \\\n",
    "    --label-map data/skeleton/e19_ciis_label_map.txt\n",
    "\n",
    "import moviepy.editor as mpy\n",
    "from mmaction.utils import frame_extract\n",
    "import tempfile\n",
    "\n",
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "frame_paths, vis_frames = frame_extract(out_filename, out_dir=tmp_dir.name)\n",
    "\n",
    "vid = mpy.ImageSequenceClip(vis_frames, fps=12)\n",
    "vid.write_videofile(out_filename, remove_temp=True)\n",
    "\n",
    "tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee76110-cab0-46b6-bb6a-de799da5dc4b",
   "metadata": {},
   "source": [
    "input video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d7745f8-0278-4289-88bd-5a014ddfb6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=50% controls autoplay loop><source src=data/test_video/e19/frontal-statis_satu-objek_aksi-tetap.mp4></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src='+video+'></video>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87f8553-4133-418b-972c-9cc729871377",
   "metadata": {},
   "source": [
    "output video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96005e2e-901e-45ca-99fb-67cd10960249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=50% controls autoplay loop><source src=data/test_video/e19/frontal-statis_satu-objek_aksi-tetap_out.mp4></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src='+out_filename+'></video>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ebca12-2f2e-4bcb-9e10-f23c26706fcb",
   "metadata": {},
   "source": [
    "## Testing via bash - Spatio-Temporal Recognition [E'20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d541ce2f-026e-4c1e-aedf-4b41180467c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[                                                  ] 0/122, elapsed: 0s, ETA:10/01 02:21:33 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "10/01 02:21:33 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 122/122, 4.0 task/s, elapsed: 30s, ETA:     0s\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 122/122, 12.8 task/s, elapsed: 10s, ETA:     0s\n",
      "Use skeleton-based SpatioTemporal Action Detection\n",
      "Loads checkpoint by local backend from path: work_dirs/ciis_10_best-550/best_acc_top1_epoch_550.pth\n",
      "Performing SpatioTemporal Action Detection for each clip\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 30/30, 39.2 task/s, elapsed: 1s, ETA:     0sMoviepy - Building video data/test_video/e20/30dAerial-dinamis_multi-objek_aksi-berubah_out.mp4.\n",
      "Moviepy - Writing video data/test_video/e20/30dAerial-dinamis_multi-objek_aksi-berubah_out.mp4\n",
      "\n",
      "Moviepy - Done !                                                                \n",
      "Moviepy - video ready data/test_video/e20/30dAerial-dinamis_multi-objek_aksi-berubah_out.mp4\n"
     ]
    }
   ],
   "source": [
    "!python demo_sp-te_ac-re.py \\\n",
    "    --video data/test_video/e20/30dAerial-dinamis_multi-objek_aksi-berubah.mp4 \\\n",
    "    --out-filename data/test_video/e20/30dAerial-dinamis_multi-objek_aksi-berubah_out.mp4 \\\n",
    "\\\n",
    "    --det-config mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py \\\n",
    "    --det-checkpoint http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth \\\n",
    "    --det-score-thr 0.9 \\\n",
    "\\\n",
    "    --pose-config mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py \\\n",
    "    --pose-checkpoint https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth \\\n",
    "\\\n",
    "    --skeleton-config configs/skeleton/posec3d/ciis_10.py \\\n",
    "    --skeleton-stdet-checkpoint work_dirs/ciis_10_best-550/best_acc_top1_epoch_550.pth \\\n",
    "    --action-score-thr 0.75 \\\n",
    "    --label-map-stdet data/skeleton/ciis_label_map.txt \\\n",
    "\\\n",
    "    --predict-stepsize 4 \\\n",
    "    --output-fps 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd81ab2f-f705-4e30-be2d-cda0d69ec702",
   "metadata": {},
   "source": [
    "input video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad587218-846b-4b5b-b016-083302936b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=50% controls autoplay loop><source src=\"data/test_video/e20/30dAerial-dinamis_multi-objek_aksi-berubah.mp4\"></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"data/test_video/e20/30dAerial-dinamis_multi-objek_aksi-berubah.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b7199a-f69d-46d5-b40b-658f61d365c4",
   "metadata": {},
   "source": [
    "output video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f98e148-3cf2-46a8-a265-f8aa7c65649f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=50% controls autoplay loop><source src=\"data/test_video/e20/30dAerial-dinamis_multi-objek_aksi-berubah_out.mp4\"></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"data/test_video/e20/30dAerial-dinamis_multi-objek_aksi-berubah_out.mp4\"></video>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721728ce-4274-4789-8c36-f4f600f66dfa",
   "metadata": {},
   "source": [
    "## Testing on python - Spatio-Temporal Recognition [E'20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cc52445-850c-427d-9925-2320af1eace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "\n",
    "import copy as cp\n",
    "import tempfile\n",
    "\n",
    "import cv2\n",
    "import mmcv\n",
    "import mmengine\n",
    "import numpy as np\n",
    "import torch\n",
    "# from mmengine import DictAction\n",
    "\n",
    "from mmaction.apis import (detection_inference, inference_recognizer,\n",
    "                           init_recognizer, pose_inference)\n",
    "from mmaction.registry import VISUALIZERS\n",
    "from mmaction.utils import frame_extract\n",
    "\n",
    "import moviepy.editor as mpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba4f40d-f9a6-4da0-8764-1ec32ae12c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTFACE = cv2.FONT_HERSHEY_DUPLEX\n",
    "FONTSCALE = 1.25\n",
    "\n",
    "THICKNESS = 2  # int\n",
    "LINETYPE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4145efa8-5d3b-4b3d-a4ad-0b4c578c3cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex2color(h):\n",
    "    \"\"\"Convert the 6-digit hex string to tuple of 3 int value (RGB)\"\"\"\n",
    "    return (int(h[:2], 16), int(h[2:4], 16), int(h[4:], 16))\n",
    "\n",
    "\n",
    "PLATEBLUE = '03045e-023e8a-0077b6-0096c7-00b4d8-48cae4'\n",
    "PLATEBLUE = PLATEBLUE.split('-')\n",
    "PLATEBLUE = [hex2color(h) for h in PLATEBLUE]\n",
    "\n",
    "\n",
    "def visualize(pose_config,\n",
    "              frames,\n",
    "              annotations,\n",
    "              pose_data_samples,\n",
    "              action_result,\n",
    "              plate=PLATEBLUE,\n",
    "              max_num=5):\n",
    "    \"\"\"Visualize frames with predicted annotations.\n",
    "\n",
    "    Args:\n",
    "        frames (list[np.ndarray]): Frames for visualization, note that\n",
    "            len(frames) % len(annotations) should be 0.\n",
    "        annotations (list[list[tuple]]): The predicted spatio-temporal\n",
    "            detection results.\n",
    "        pose_data_samples (list[list[PoseDataSample]): The pose results.\n",
    "        action_result (str): The predicted action recognition results.\n",
    "        pose_model (nn.Module): The constructed pose model.\n",
    "        plate (str): The plate used for visualization. Default: PLATEBLUE.\n",
    "        max_num (int): Max number of labels to visualize for a person box.\n",
    "            Default: 5.\n",
    "\n",
    "    Returns:\n",
    "        list[np.ndarray]: Visualized frames.\n",
    "    \"\"\"\n",
    "\n",
    "    assert max_num + 1 <= len(plate)\n",
    "    frames_ = cp.deepcopy(frames)\n",
    "    frames_ = [mmcv.imconvert(f, 'bgr', 'rgb') for f in frames_]\n",
    "    nf, na = len(frames), len(annotations)\n",
    "    assert nf % na == 0\n",
    "    nfpa = len(frames) // len(annotations)\n",
    "    anno = None\n",
    "    h, w, _ = frames[0].shape\n",
    "    scale_ratio = np.array([w, h, w, h])\n",
    "\n",
    "    # add pose results\n",
    "    if pose_data_samples:\n",
    "        pose_config = mmengine.Config.fromfile(pose_config)\n",
    "        visualizer = VISUALIZERS.build(pose_config.visualizer | {'line_width':5, 'bbox_color':(101,193,255), 'radius': 8})  # https://mmpose.readthedocs.io/en/latest/api.html#mmpose.visualization.PoseLocalVisualizer\n",
    "        visualizer.set_dataset_meta(pose_data_samples[0].dataset_meta)\n",
    "        for i, (d, f) in enumerate(zip(pose_data_samples, frames_)):\n",
    "            visualizer.add_datasample(\n",
    "                'result',\n",
    "                f,\n",
    "                data_sample=d,\n",
    "                draw_gt=False,\n",
    "                draw_heatmap=False,\n",
    "                draw_bbox=True,\n",
    "                draw_pred=True,\n",
    "                show=False,\n",
    "                wait_time=0,\n",
    "                out_file=None,\n",
    "                kpt_thr=0.3)\n",
    "            frames_[i] = visualizer.get_image()\n",
    "\n",
    "    for i in range(na):\n",
    "        anno = annotations[i]\n",
    "        if anno is None:\n",
    "            continue\n",
    "        for j in range(nfpa):\n",
    "            ind = i * nfpa + j\n",
    "            frame = frames_[ind]\n",
    "\n",
    "            # add spatio-temporal action detection results\n",
    "            for ann in anno:\n",
    "                box = ann[0]\n",
    "                label = ann[1]\n",
    "                if not len(label):\n",
    "                    continue\n",
    "                score = ann[2]\n",
    "                box = (box * scale_ratio).astype(np.int64)\n",
    "                st, ed = tuple(box[:2]), tuple(box[2:])\n",
    "                if not pose_data_samples:\n",
    "                    cv2.rectangle(frame, st, ed, plate[0], 2)\n",
    "\n",
    "                for k, lb in enumerate(label):\n",
    "                    if k >= max_num:\n",
    "                        break\n",
    "                    text = abbrev(lb)\n",
    "                    text = ': '.join([text, f'{(score[k]*100):.1f}%'])\n",
    "                    location = (0 + st[0], 18 + k * 18 + st[1])\n",
    "                    textsize = cv2.getTextSize(text, FONTFACE, FONTSCALE,\n",
    "                                               THICKNESS)[0]\n",
    "                    textwidth = textsize[0]\n",
    "                    diag0 = (location[0] + textwidth, location[1] - 14)\n",
    "                    diag1 = (location[0], location[1] + 2)\n",
    "                    cv2.rectangle(frame, diag0, diag1, plate[k + 1], -1)\n",
    "                    bahaya = ['melempar', 'membidik senapan', 'membidik pistol', 'memukul', 'menendang', 'menusuk']\n",
    "                    FONTCOLOR = (255, 0, 0) if lb in bahaya else (255, 255, 255)\n",
    "                    cv2.putText(frame, text, location, FONTFACE, FONTSCALE,\n",
    "                                FONTCOLOR, THICKNESS, LINETYPE)\n",
    "\n",
    "    return frames_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df020acf-23a4-4537-b117-c802991bb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = 'data/test_video/e20/30dAerial-dinamis_satu-objek_aksi-berubah.mp4'\n",
    "out_filename = 'data/test_video/e20/30dAerial-dinamis_satu-objek_aksi-berubah_out.mp4'\n",
    "\n",
    "# human detection config\n",
    "det_config = 'mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py'\n",
    "det_checkpoint = 'http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth'\n",
    "det_score_thr = 0.9\n",
    "#det_cat_id = 0\n",
    "\n",
    "# pose estimation config\n",
    "pose_config = 'mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py'\n",
    "pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth'\n",
    "\n",
    "# skeleton-based spatio-temporal action classification config\n",
    "skeleton_config = \"configs/skeleton/posec3d/ciis_10.py\"\n",
    "skeleton_stdet_checkpoint = \"work_dirs/ciis_10_best-550/best_acc_top1_epoch_550.pth\"\n",
    "action_score_thr = 0.75\n",
    "label_map_stdet = \"data/skeleton/ciis_label_map.txt\"\n",
    "\n",
    "predict_stepsize = 4  # give out a spatio-temporal detection prediction per n frames\n",
    "output_stepsize = 1  # show one frame per n frames in the demo, we should have: predict_stepsize % output_stepsize == 0\n",
    "output_fps = 12  # the fps of demo video output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c43af11-3b1c-423e-95cf-d3dbb8c42493",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f38f52a-c759-4aa3-9081-c8535eca0007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description='MMAction2 demo')\n",
    "#     parser.add_argument(\n",
    "#         '--cfg-options',\n",
    "#         nargs='+',\n",
    "#         action=DictAction,\n",
    "#         default={},\n",
    "#         help='override some settings in the used config, the key-value pair '\n",
    "#         'in xxx=yyy format will be merged into config file. For example, '\n",
    "#         \"'--cfg-options model.backbone.depth=18 model.backbone.with_cp=True'\")\n",
    "#     args = parser.parse_args()\n",
    "#     return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54b10072-8152-490c-b05b-961873a676ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_map(file_path):\n",
    "    \"\"\"Load Label Map.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The file path of label map.\n",
    "\n",
    "    Returns:\n",
    "        dict: The label map (int -> label name).\n",
    "    \"\"\"\n",
    "    lines = open(file_path).readlines()\n",
    "    lines = [x.strip().split(': ') for x in lines]\n",
    "    return {int(x[0]): x[1] for x in lines}\n",
    "\n",
    "\n",
    "def abbrev(name):\n",
    "    \"\"\"Get the abbreviation of label name:\n",
    "\n",
    "    'take (an object) from (a person)' -> 'take ... from ...'\n",
    "    \"\"\"\n",
    "    while name.find('(') != -1:\n",
    "        st, ed = name.find('('), name.find(')')\n",
    "        name = name[:st] + '...' + name[ed + 1:]\n",
    "    return name\n",
    "\n",
    "\n",
    "def pack_result(human_detection, result, img_h, img_w):\n",
    "    \"\"\"Short summary.\n",
    "\n",
    "    Args:\n",
    "        human_detection (np.ndarray): Human detection result.\n",
    "        result (type): The predicted label of each human proposal.\n",
    "        img_h (int): The image height.\n",
    "        img_w (int): The image width.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple of human proposal, label name and label score.\n",
    "    \"\"\"\n",
    "    human_detection[:, 0::2] /= img_w\n",
    "    human_detection[:, 1::2] /= img_h\n",
    "    results = []\n",
    "    if result is None:\n",
    "        return None\n",
    "    for prop, res in zip(human_detection, result):\n",
    "        res.sort(key=lambda x: -x[1])\n",
    "        results.append(\n",
    "            (prop.data.cpu().numpy(), [x[0] for x in res], [x[1]\n",
    "                                                            for x in res]))\n",
    "    return results\n",
    "\n",
    "\n",
    "def expand_bbox(bbox, h, w, ratio=1.25):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    center_x = (x1 + x2) // 2\n",
    "    center_y = (y1 + y2) // 2\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "\n",
    "    square_l = max(width, height)\n",
    "    new_width = new_height = square_l * ratio\n",
    "\n",
    "    new_x1 = max(0, int(center_x - new_width / 2))\n",
    "    new_x2 = min(int(center_x + new_width / 2), w)\n",
    "    new_y1 = max(0, int(center_y - new_height / 2))\n",
    "    new_y2 = min(int(center_y + new_height / 2), h)\n",
    "    return (new_x1, new_y1, new_x2, new_y2)\n",
    "\n",
    "\n",
    "def cal_iou(box1, box2):\n",
    "    xmin1, ymin1, xmax1, ymax1 = box1\n",
    "    xmin2, ymin2, xmax2, ymax2 = box2\n",
    "\n",
    "    s1 = (xmax1 - xmin1) * (ymax1 - ymin1)\n",
    "    s2 = (xmax2 - xmin2) * (ymax2 - ymin2)\n",
    "\n",
    "    xmin = max(xmin1, xmin2)\n",
    "    ymin = max(ymin1, ymin2)\n",
    "    xmax = min(xmax1, xmax2)\n",
    "    ymax = min(ymax1, ymax2)\n",
    "\n",
    "    w = max(0, xmax - xmin)\n",
    "    h = max(0, ymax - ymin)\n",
    "    intersect = w * h\n",
    "    union = s1 + s2 - intersect\n",
    "    iou = intersect / union\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def skeleton_based_stdet(predict_stepsize, skeleton_config, skeleton_stdet_checkpoint, device, action_score_thr, label_map, human_detections, pose_results,\n",
    "                         num_frame, clip_len, frame_interval, h, w):\n",
    "    window_size = clip_len * frame_interval\n",
    "    assert clip_len % 2 == 0, 'We would like to have an even clip_len'\n",
    "    timestamps = np.arange(window_size // 2, num_frame + 1 - window_size // 2,\n",
    "                           predict_stepsize)\n",
    "\n",
    "    skeleton_config = mmengine.Config.fromfile(skeleton_config)\n",
    "    num_class = max(label_map.keys()) + 1  # for CIIS dataset (9 + 1) == len(label_map)\n",
    "    skeleton_config.model.cls_head.num_classes = num_class\n",
    "    skeleton_stdet_model = init_recognizer(skeleton_config,\n",
    "                                           skeleton_stdet_checkpoint,\n",
    "                                           device)\n",
    "\n",
    "    skeleton_predictions = []\n",
    "\n",
    "    print('Performing SpatioTemporal Action Detection for each clip')\n",
    "    prog_bar = mmengine.ProgressBar(len(timestamps))\n",
    "    for timestamp in timestamps:\n",
    "        proposal = human_detections[timestamp - 1]\n",
    "        if proposal.shape[0] == 0:  # no people detected\n",
    "            skeleton_predictions.append(None)\n",
    "            continue\n",
    "\n",
    "        start_frame = timestamp - (clip_len // 2 - 1) * frame_interval\n",
    "        frame_inds = start_frame + np.arange(0, window_size, frame_interval)\n",
    "        frame_inds = list(frame_inds - 1)\n",
    "        num_frame = len(frame_inds)  # 30\n",
    "\n",
    "        pose_result = [pose_results[ind] for ind in frame_inds]\n",
    "\n",
    "        skeleton_prediction = []\n",
    "        for i in range(proposal.shape[0]):  # num_person\n",
    "            skeleton_prediction.append([])\n",
    "\n",
    "            fake_anno = dict(\n",
    "                frame_dict='',\n",
    "                label=-1,\n",
    "                img_shape=(h, w),\n",
    "                origin_shape=(h, w),\n",
    "                start_index=0,\n",
    "                modality='Pose',\n",
    "                num_clips=1,\n",
    "                clip_len=clip_len,\n",
    "                total_frames=num_frame)\n",
    "            num_person = 1\n",
    "\n",
    "            num_keypoint = 17\n",
    "            keypoint = np.zeros(\n",
    "                (num_person, num_frame, num_keypoint, 2))  # M T V 2\n",
    "            keypoint_score = np.zeros(\n",
    "                (num_person, num_frame, num_keypoint))  # M T V\n",
    "\n",
    "            # pose matching\n",
    "            person_bbox = proposal[i][:4]  #x1, y1, x2, y2\n",
    "            area = expand_bbox(person_bbox, h, w)\n",
    "\n",
    "            for j, poses in enumerate(pose_result):  # num_frame\n",
    "                max_iou = float('-inf')\n",
    "                index = -1\n",
    "                if len(poses['keypoints']) == 0:\n",
    "                    continue\n",
    "                for k, bbox in enumerate(poses['bboxes']):  # num_person\n",
    "                    iou = cal_iou(bbox, area)\n",
    "                    if max_iou < iou:  # if isBelong\n",
    "                        index = k\n",
    "                        max_iou = iou\n",
    "                keypoint[0, j] = poses['keypoints'][index]\n",
    "                keypoint_score[0, j] = poses['keypoint_scores'][index]\n",
    "\n",
    "            fake_anno['keypoint'] = keypoint\n",
    "            fake_anno['keypoint_score'] = keypoint_score\n",
    "\n",
    "            output = inference_recognizer(skeleton_stdet_model, fake_anno)\n",
    "            # for multi-label recognition\n",
    "            score = output.pred_score.tolist()\n",
    "            for k in range(len(score)):  # 10\n",
    "                if k not in label_map:\n",
    "                    continue\n",
    "                if score[k] > action_score_thr:\n",
    "                    skeleton_prediction[i].append((label_map[k], score[k]))\n",
    "\n",
    "            # crop the image -> resize -> extract pose -> as input for poseC3D\n",
    "\n",
    "        skeleton_predictions.append(skeleton_prediction)\n",
    "        prog_bar.update()\n",
    "\n",
    "    return timestamps, skeleton_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31a3917c-9766-4939-988c-db068e5fcf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#args = parse_args()\n",
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "frame_paths, original_frames = frame_extract(\n",
    "    video, 720, out_dir=tmp_dir.name)\n",
    "num_frame = len(frame_paths)\n",
    "h, w, _ = original_frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "244ad1a3-d9a5-49c0-abe5-8c365a291b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[                                                  ] 0/122, elapsed: 0s, ETA:10/01 02:18:25 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "10/01 02:18:25 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 122/122, 4.0 task/s, elapsed: 31s, ETA:     0s\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 122/122, 15.0 task/s, elapsed: 8s, ETA:     0s\n"
     ]
    }
   ],
   "source": [
    "# get Human detection results\n",
    "human_detections, _ = detection_inference(\n",
    "    det_config,\n",
    "    det_checkpoint,\n",
    "    frame_paths,\n",
    "    det_score_thr,\n",
    "    device=device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# get Pose estimation results\n",
    "pose_datasample = None\n",
    "pose_results, pose_datasample = pose_inference(\n",
    "    pose_config,\n",
    "    pose_checkpoint,\n",
    "    frame_paths,\n",
    "    human_detections,\n",
    "    device=device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1efb83f-385b-4a75-8072-3f3295680b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize frames to shortside 256\n",
    "# new_w, new_h = mmcv.rescale_size((w, h), (480, np.Inf))\n",
    "new_w, new_h = w, h\n",
    "# frames = [mmcv.imresize(img, (new_w, new_h)) for img in original_frames]\n",
    "frames = original_frames\n",
    "w_ratio, h_ratio = new_w / w, new_h / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b26ea491-b1b3-4e33-bfe8-ac973c8a2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spatio-temporal detection label_map\n",
    "stdet_label_map = load_label_map(label_map_stdet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77ca5495-aca1-4d5a-8390-28beb907be26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use skeleton-based SpatioTemporal Action Detection\n",
      "Loads checkpoint by local backend from path: work_dirs/ciis_10_best-550/best_acc_top1_epoch_550.pth\n",
      "Performing SpatioTemporal Action Detection for each clip\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 30/30, 63.1 task/s, elapsed: 0s, ETA:     0s"
     ]
    }
   ],
   "source": [
    "stdet_preds = None\n",
    "\n",
    "print('Use skeleton-based SpatioTemporal Action Detection')\n",
    "# clip_len, frame_interval = 30, 1\n",
    "clip_len, frame_interval = predict_stepsize, 1\n",
    "timestamps, stdet_preds = skeleton_based_stdet(predict_stepsize,\n",
    "                                               skeleton_config,\n",
    "                                               skeleton_stdet_checkpoint,\n",
    "                                               device,\n",
    "                                               action_score_thr,\n",
    "                                               stdet_label_map,\n",
    "                                               human_detections,\n",
    "                                               pose_results, num_frame,\n",
    "                                               clip_len,\n",
    "                                               frame_interval, h, w)\n",
    "for i in range(len(human_detections)):\n",
    "    det = human_detections[i]\n",
    "    det[:, 0:4:2] *= w_ratio\n",
    "    det[:, 1:4:2] *= h_ratio\n",
    "    human_detections[i] = torch.from_numpy(det[:, :4]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dbf9264-8a9e-4ce1-bb45-1aec960e3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_results = []\n",
    "for timestamp, prediction in zip(timestamps, stdet_preds):\n",
    "    human_detection = human_detections[timestamp - 1]\n",
    "    stdet_results.append(\n",
    "        pack_result(human_detection, prediction, new_h, new_w))\n",
    "\n",
    "def dense_timestamps(timestamps, n):\n",
    "    \"\"\"Make it nx frames.\"\"\"\n",
    "    old_frame_interval = (timestamps[1] - timestamps[0])\n",
    "    start = timestamps[0] - old_frame_interval / n * (n - 1) / 2\n",
    "    new_frame_inds = np.arange(\n",
    "        len(timestamps) * n) * old_frame_interval / n + start\n",
    "    return new_frame_inds.astype(np.int64)\n",
    "\n",
    "dense_n = int(predict_stepsize / output_stepsize)\n",
    "# output_timestamps = dense_timestamps(timestamps, dense_n)\n",
    "output_timestamps = dense_timestamps(timestamps, dense_n) + 1\n",
    "frames = [\n",
    "    cv2.imread(frame_paths[timestamp - 1])\n",
    "    # cv2.imread(\"../../../Downloads/1280x720-white-solid-color-background.jpg\")\n",
    "    for timestamp in output_timestamps\n",
    "]\n",
    "\n",
    "pose_datasample = [\n",
    "    pose_datasample[timestamp - 1] for timestamp in output_timestamps\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e86754fd-de74-49f2-986b-19289918c520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video data/test_video/e20/30dAerial-dinamis_satu-objek_aksi-berubah_out.mp4.\n",
      "Moviepy - Writing video data/test_video/e20/30dAerial-dinamis_satu-objek_aksi-berubah_out.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready data/test_video/e20/30dAerial-dinamis_satu-objek_aksi-berubah_out.mp4\n"
     ]
    }
   ],
   "source": [
    "vis_frames = visualize(pose_config, frames, stdet_results, pose_datasample,\n",
    "                       None)\n",
    "vid = mpy.ImageSequenceClip(vis_frames, fps=output_fps)\n",
    "vid.write_videofile(out_filename)\n",
    "tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1b95203-b9bf-4b0f-9c66-2d562b96fd7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=50% controls autoplay loop><source src=\"data/test_video/e20/30dAerial-dinamis_satu-objek_aksi-berubah.mp4\"></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"'+video+'\"></video>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfb9f4e8-314d-4de2-9a3e-4558146e1972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=50% controls autoplay loop><source src=\"data/test_video/e20/30dAerial-dinamis_satu-objek_aksi-berubah_out.mp4\"></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<video width=50% controls autoplay loop><source src=\"'+out_filename+'\"></video>')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
