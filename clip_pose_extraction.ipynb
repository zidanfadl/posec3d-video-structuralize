{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f660d7-20e5-45cd-9e8e-47a0f9d5b159",
   "metadata": {},
   "source": [
    "# Pose Extraction + Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a971dbaa-8cb8-4b55-a91e-5c8ecfc7a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) CIIS-Lab. All rights reserved.\n",
    "import os.path as osp\n",
    "\n",
    "import copy as cp\n",
    "import tempfile\n",
    "\n",
    "import cv2\n",
    "import mmcv\n",
    "import mmengine\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from mmaction.apis import (detection_inference,\n",
    "                           # inference_recognizer, init_recognizer,\n",
    "                           pose_inference)\n",
    "from mmaction.registry import VISUALIZERS\n",
    "from mmaction.utils import frame_extract\n",
    "\n",
    "import moviepy.editor as mpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901f90fa-6681-4fda-b96d-78311755cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTFACE = cv2.FONT_HERSHEY_DUPLEX\n",
    "FONTSCALE = 1.25\n",
    "\n",
    "THICKNESS = 2  # int\n",
    "LINETYPE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7fa058-e46c-4ff9-abdb-06082817c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex2color(h):\n",
    "    \"\"\"Convert the 6-digit hex string to tuple of 3 int value (RGB)\"\"\"\n",
    "    return (int(h[:2], 16), int(h[2:4], 16), int(h[4:], 16))\n",
    "\n",
    "PLATEBLUE = '03045e-023e8a-0077b6-0096c7-00b4d8-48cae4'\n",
    "PLATEBLUE = PLATEBLUE.split('-')\n",
    "PLATEBLUE = [hex2color(h) for h in PLATEBLUE]\n",
    "\n",
    "\n",
    "def visualize(pose_config,\n",
    "              frames,\n",
    "              annotations,\n",
    "              pose_data_samples,\n",
    "              action_result,\n",
    "              plate=PLATEBLUE,\n",
    "              max_num=5):\n",
    "    \"\"\"Visualize frames with predicted annotations.\n",
    "\n",
    "    Args:\n",
    "        frames (list[np.ndarray]): Frames for visualization, note that\n",
    "            len(frames) % len(annotations) should be 0.\n",
    "        annotations (list[list[tuple]]): The predicted spatio-temporal\n",
    "            detection results.\n",
    "        pose_data_samples (list[list[PoseDataSample]): The pose results.\n",
    "        action_result (str): The predicted action recognition results.\n",
    "        pose_model (nn.Module): The constructed pose model.\n",
    "        plate (str): The plate used for visualization. Default: PLATEBLUE.\n",
    "        max_num (int): Max number of labels to visualize for a person box.\n",
    "            Default: 5.\n",
    "\n",
    "    Returns:\n",
    "        list[np.ndarray]: Visualized frames.\n",
    "    \"\"\"\n",
    "\n",
    "    assert max_num + 1 <= len(plate)\n",
    "    frames_ = cp.deepcopy(frames)\n",
    "    frames_ = [mmcv.imconvert(f, 'bgr', 'rgb') for f in frames_]\n",
    "    nf, na = len(frames), len(annotations)\n",
    "    assert nf % na == 0\n",
    "    nfpa = len(frames) // len(annotations)\n",
    "    anno = None\n",
    "    h, w, _ = frames[0].shape\n",
    "    scale_ratio = np.array([w, h, w, h])\n",
    "\n",
    "    # add pose results\n",
    "    if pose_data_samples:\n",
    "        pose_config = mmengine.Config.fromfile(pose_config)\n",
    "        visualizer = VISUALIZERS.build(pose_config.visualizer | {'line_width':5, 'bbox_color':(101,193,255), 'radius': 8})  # https://mmpose.readthedocs.io/en/latest/api.html#mmpose.visualization.PoseLocalVisualizer\n",
    "        visualizer.set_dataset_meta(pose_data_samples[0].dataset_meta)\n",
    "        for i, (d, f) in enumerate(zip(pose_data_samples, frames_)):\n",
    "            visualizer.add_datasample(\n",
    "                'result',\n",
    "                f,\n",
    "                data_sample=d,\n",
    "                draw_gt=False,\n",
    "                draw_heatmap=False,\n",
    "                draw_bbox=True,\n",
    "                draw_pred=True,\n",
    "                show=False,\n",
    "                wait_time=0,\n",
    "                out_file=None,\n",
    "                kpt_thr=0.3)\n",
    "            frames_[i] = visualizer.get_image()\n",
    "\n",
    "    for i in range(na):\n",
    "        anno = annotations[i]\n",
    "        if anno is None:\n",
    "            continue\n",
    "        for j in range(nfpa):\n",
    "            ind = i * nfpa + j\n",
    "            frame = frames_[ind]\n",
    "\n",
    "            # add spatio-temporal action detection results\n",
    "            for ann in anno:\n",
    "                box = ann[0]\n",
    "                label = ann[1]\n",
    "                if not len(label):\n",
    "                    continue\n",
    "                score = ann[2]\n",
    "                box = (box * scale_ratio).astype(np.int64)\n",
    "                st, ed = tuple(box[:2]), tuple(box[2:])\n",
    "                if not pose_data_samples:\n",
    "                    cv2.rectangle(frame, st, ed, plate[0], 2)\n",
    "\n",
    "                for k, lb in enumerate(label):\n",
    "                    if k >= max_num:\n",
    "                        break\n",
    "                    text = abbrev(lb)\n",
    "                    text = ': '.join([text, f'{score[k]:.3f}'])\n",
    "                    location = (0 + st[0], 18 + k * 18 + st[1])\n",
    "                    textsize = cv2.getTextSize(text, FONTFACE, FONTSCALE,\n",
    "                                               THICKNESS)[0]\n",
    "                    textwidth = textsize[0]\n",
    "                    diag0 = (location[0] + textwidth, location[1] - 14)\n",
    "                    diag1 = (location[0], location[1] + 2)\n",
    "                    cv2.rectangle(frame, diag0, diag1, plate[k + 1], -1)\n",
    "                    FONTCOLOR = (255, 0, 0)\n",
    "                    cv2.putText(frame, text, location, FONTFACE, FONTSCALE,\n",
    "                                FONTCOLOR, THICKNESS, LINETYPE)\n",
    "\n",
    "    return frames_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b297f190-39ef-42dd-9f12-e669e029db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = 'data/skeleton/raw/30d_1s1.mp4'\n",
    "ann_filename = 'data/skeleton/to-anno/30d_1s1.csv'\n",
    "pkl_filename = 'data/skeleton/to-anno/30d_1s1.pkl'\n",
    "out_filename = 'data/skeleton/to-anno/30d_1s1_gt.mp4'\n",
    "\n",
    "# human detection config\n",
    "det_config = 'mmaction2/demo/demo_configs/faster-rcnn_r50_fpn_2x_coco_infer.py'\n",
    "det_checkpoint = 'http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth'\n",
    "det_score_thr = 0.9\n",
    "#det_cat_id = 0\n",
    "\n",
    "# pose estimation config\n",
    "pose_config = 'mmaction2/demo/demo_configs/td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py'\n",
    "pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth'\n",
    "\n",
    "# use skeleton-based method\n",
    "use_skeleton_stdet = True\n",
    "use_skeleton_recog = True\n",
    "\n",
    "# skeleton-based spatio-temporal action classification config\n",
    "label_map_stdet = \"data/skeleton/ciis_label_map.txt\"\n",
    "\n",
    "predict_stepsize = 4  # must even int, give out a spatio-temporal detection prediction per n frames\n",
    "output_stepsize = 1  # show one frame per n frames in the demo, we should have: predict_stepsize % output_stepsize == 0, speedUp/slowDown video output\n",
    "output_fps = 12  # the fps of demo video output, will speedUp/slowDown video output, must equal to (video_input_fps/output_stepsize) to get normal speed\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23467484-3784-4272-b862-bb1fcee3bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_label_map(file_path):\n",
    "#     \"\"\"Load Label Map.\n",
    "\n",
    "#     Args:\n",
    "#         file_path (str): The file path of label map.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: The label map (int -> label name).\n",
    "#     \"\"\"\n",
    "#     lines = open(file_path).readlines()\n",
    "#     lines = [x.strip().split(': ') for x in lines]\n",
    "#     return {int(x[0]): x[1] for x in lines}\n",
    "\n",
    "\n",
    "def abbrev(name):\n",
    "    \"\"\"Get the abbreviation of label name:\n",
    "\n",
    "    'take (an object) from (a person)' -> 'take ... from ...'\n",
    "    \"\"\"\n",
    "    while name.find('(') != -1:\n",
    "        st, ed = name.find('('), name.find(')')\n",
    "        name = name[:st] + '...' + name[ed + 1:]\n",
    "    return name\n",
    "\n",
    "def pack_result(human_detection, result, img_h, img_w):\n",
    "    \"\"\"Short summary.\n",
    "\n",
    "    Args:\n",
    "        human_detection (np.ndarray): Human detection result.\n",
    "        result (type): The predicted label of each human proposal.\n",
    "        img_h (int): The image height.\n",
    "        img_w (int): The image width.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple of human proposal, label name and label score.\n",
    "    \"\"\"\n",
    "    human_detection[:, 0::2] /= img_w\n",
    "    human_detection[:, 1::2] /= img_h\n",
    "    results = []\n",
    "    if result is None:\n",
    "        return None\n",
    "    for prop, res in zip(human_detection, result):\n",
    "        res.sort(key=lambda x: -x[1])\n",
    "        results.append(\n",
    "            (prop.data.cpu().numpy(), [x[0] for x in res], [x[1]\n",
    "                                                            for x in res]))\n",
    "    return results\n",
    "\n",
    "\n",
    "def expand_bbox(bbox, h, w, ratio=1.25):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    center_x = (x1 + x2) // 2\n",
    "    center_y = (y1 + y2) // 2\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "\n",
    "    square_l = max(width, height)\n",
    "    new_width = new_height = square_l * ratio\n",
    "\n",
    "    new_x1 = max(0, int(center_x - new_width / 2))\n",
    "    new_x2 = min(int(center_x + new_width / 2), w)\n",
    "    new_y1 = max(0, int(center_y - new_height / 2))\n",
    "    new_y2 = min(int(center_y + new_height / 2), h)\n",
    "    return (new_x1, new_y1, new_x2, new_y2)\n",
    "\n",
    "\n",
    "def cal_iou(box1, box2):\n",
    "    xmin1, ymin1, xmax1, ymax1 = box1\n",
    "    xmin2, ymin2, xmax2, ymax2 = box2\n",
    "\n",
    "    s1 = (xmax1 - xmin1) * (ymax1 - ymin1)\n",
    "    s2 = (xmax2 - xmin2) * (ymax2 - ymin2)\n",
    "\n",
    "    xmin = max(xmin1, xmin2)\n",
    "    ymin = max(ymin1, ymin2)\n",
    "    xmax = min(xmax1, xmax2)\n",
    "    ymax = min(ymax1, ymax2)\n",
    "\n",
    "    w = max(0, xmax - xmin)\n",
    "    h = max(0, ymax - ymin)\n",
    "    intersect = w * h\n",
    "    union = s1 + s2 - intersect\n",
    "    iou = intersect / union\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "# clip_pose_extraction\n",
    "def skeleton_based_stdet(predict_stepsize, video,\n",
    "                         # skeleton_config, skeleton_stdet_checkpoint, device, action_score_thr, label_map,\n",
    "                         human_detections, pose_results, num_frame, clip_len, frame_interval, h, w):\n",
    "    window_size = clip_len * frame_interval\n",
    "    assert clip_len % 2 == 0, 'We would like to have an even clip_len'\n",
    "    timestamps = np.arange(window_size // 2, num_frame + 1 - window_size // 2,\n",
    "                           predict_stepsize)\n",
    "\n",
    "    # skeleton_config = mmengine.Config.fromfile(skeleton_config)\n",
    "    # num_class = max(label_map.keys()) + 1  # for AVA dataset (81)\n",
    "    # skeleton_config.model.cls_head.num_classes = num_class\n",
    "    # skeleton_stdet_model = init_recognizer(skeleton_config,\n",
    "    #                                        skeleton_stdet_checkpoint,\n",
    "    #                                        device)\n",
    "\n",
    "    skeleton_predictions = []\n",
    "    skeleton_datasets = []\n",
    "\n",
    "    print('Building skeleton datasets from existing keypoint data for each clip')\n",
    "    prog_bar = mmengine.ProgressBar(len(timestamps))\n",
    "    for timestamp in timestamps:  # iterate each clip\n",
    "        proposal = human_detections[timestamp - 1] # get bboxes for persons in timestamp (first frame of clip)\n",
    "        if proposal.shape[0] == 0:  # no people detected\n",
    "            skeleton_predictions.append(None)\n",
    "            continue\n",
    "\n",
    "        start_frame = timestamp - (clip_len // 2 - 1) * frame_interval\n",
    "        frame_inds = start_frame + np.arange(0, window_size, frame_interval)\n",
    "        frame_inds = list(frame_inds - 1)\n",
    "        num_frame = len(frame_inds)  # 30\n",
    "\n",
    "        pose_result = [pose_results[ind] for ind in frame_inds]  # grouping frames poses for each clip\n",
    "\n",
    "        skeleton_prediction = []\n",
    "        for i in range(proposal.shape[0]):  # num_person  # iterate each bbox in timestamp (first frame of clip)\n",
    "            skeleton_prediction.append([])\n",
    "\n",
    "            fake_anno = dict(\n",
    "                frame_dir=osp.splitext(osp.basename(video))[0]+\"_\"+str(timestamp+(i+1)*0.001),\n",
    "                label=-1,\n",
    "                img_shape=(h, w),\n",
    "                original_shape=(h, w),\n",
    "                num_clips=1,\n",
    "                total_frames=num_frame\n",
    "            )\n",
    "            num_person = 1\n",
    "\n",
    "            num_keypoint = 17\n",
    "            keypoint = np.zeros(\n",
    "                (num_person, num_frame, num_keypoint, 2))  # M T V 2\n",
    "            keypoint_score = np.zeros(\n",
    "                (num_person, num_frame, num_keypoint))  # M T V\n",
    "\n",
    "            # pose matching\n",
    "            person_bbox = proposal[i][:4]  # get bbox for a person in timestamp (first frame of clip)\n",
    "            area = expand_bbox(person_bbox, h, w)  # bbox expanded by 1.25 ratio with square shape\n",
    "\n",
    "            for j, poses in enumerate(pose_result):  # num_frame  # iterate each frame of clip\n",
    "                max_iou = float('-inf')\n",
    "                index = -1\n",
    "                if len(poses['keypoints']) == 0:\n",
    "                    continue\n",
    "                for k, bbox in enumerate(poses['bboxes']):  # iterate each bbox/pose in each frame\n",
    "                    iou = cal_iou(bbox, area)  # compare each bbox in each frame with current area (calculate_intersect/union)\n",
    "                    if max_iou < iou:\n",
    "                        index = k  # pose from the biggest intersect/union (iou) will be considered\n",
    "                        max_iou = iou\n",
    "                keypoint[0, j] = poses['keypoints'][index]\n",
    "                keypoint_score[0, j] = poses['keypoint_scores'][index]\n",
    "\n",
    "            fake_anno['keypoint'] = keypoint\n",
    "            fake_anno['keypoint_score'] = keypoint_score\n",
    "\n",
    "            skeleton_datasets.append(fake_anno)\n",
    "            # output = inference_recognizer(skeleton_stdet_model, fake_anno)\n",
    "            # # for multi-label recognition\n",
    "            # score = output.pred_score.tolist()\n",
    "            # for k in range(len(score)):  # 81\n",
    "            #     if k not in label_map:\n",
    "            #         continue\n",
    "            #     if score[k] > action_score_thr:\n",
    "            #         skeleton_prediction[i].append((label_map[k], score[k]))\n",
    "            skeleton_prediction[i].append((\"annotate!\", timestamp + (i+1)*0.001))\n",
    "\n",
    "        skeleton_predictions.append(skeleton_prediction)\n",
    "        prog_bar.update()\n",
    "\n",
    "    return timestamps, skeleton_predictions, skeleton_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596aac39-11c2-4554-ab67-832b5f39816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#args = parse_args()\n",
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "frame_paths, original_frames = frame_extract(\n",
    "    video, 720, out_dir=tmp_dir.name)\n",
    "num_frame = len(frame_paths)\n",
    "h, w, _ = original_frames[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59d5bd0-48e2-47e5-a7c9-d4261ad35873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Human detection results\n",
    "human_detections, _ = detection_inference(\n",
    "    det_config,\n",
    "    det_checkpoint,\n",
    "    frame_paths,\n",
    "    det_score_thr,\n",
    "    device=device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# get Pose estimation results\n",
    "pose_datasample = None\n",
    "pose_results, pose_datasample = pose_inference(\n",
    "    pose_config,\n",
    "    pose_checkpoint,\n",
    "    frame_paths,\n",
    "    human_detections,\n",
    "    device=device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8274ffe-a280-4584-a842-676b860dc34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_preds = None\n",
    "\n",
    "print('Use skeleton-based SpatioTemporal Action Detection')\n",
    "# clip_len, frame_interval = 30, 1\n",
    "clip_len, frame_interval = predict_stepsize, 1\n",
    "\n",
    "# clip_pose_extraction\n",
    "timestamps, stdet_preds, skeleton_datasets = skeleton_based_stdet(predict_stepsize, video,\n",
    "                                                                  # skeleton_config,\n",
    "                                                                  # skeleton_stdet_checkpoint,\n",
    "                                                                  # device,\n",
    "                                                                  # action_score_thr,\n",
    "                                                                  # stdet_label_map,\n",
    "                                                                  human_detections,\n",
    "                                                                  pose_results, num_frame,\n",
    "                                                                  clip_len,\n",
    "                                                                  frame_interval, h, w)\n",
    "for i in range(len(human_detections)):\n",
    "    det = human_detections[i]\n",
    "    # det[:, 0:4:2] *= w_ratio\n",
    "    # det[:, 1:4:2] *= h_ratio\n",
    "    det[:, 0:4:2] *= 1\n",
    "    det[:, 1:4:2] *= 1\n",
    "    human_detections[i] = torch.from_numpy(det[:, :4]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481a9465-ae72-42ef-91b9-cf09861672d9",
   "metadata": {},
   "source": [
    "## Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481a4de-843b-4dff-80ec-4fdcd773f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno = \"\"\n",
    "for clip in stdet_preds:\n",
    "    if clip == None:\n",
    "        continue\n",
    "    for person_attr in clip:\n",
    "        anno += str(person_attr[0][0]) + \",\" + str(person_attr[0][1]) + \"\\n\"\n",
    "\n",
    "with open(ann_filename,'w') as data:\n",
    "    data.write(anno)\n",
    "\n",
    "mmengine.dump(skeleton_datasets, pkl_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa487242-f475-4e5d-b382-f8c482c364d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_results = []\n",
    "for timestamp, prediction in zip(timestamps, stdet_preds):\n",
    "    human_detection = human_detections[timestamp - 1]\n",
    "    stdet_results.append(\n",
    "        pack_result(human_detection, prediction, h, w))\n",
    "\n",
    "def dense_timestamps(timestamps, n):\n",
    "    \"\"\"Make it nx frames.\"\"\"\n",
    "    old_frame_interval = (timestamps[1] - timestamps[0])\n",
    "    start = timestamps[0] - old_frame_interval / n * (n - 1) / 2\n",
    "    new_frame_inds = np.arange(\n",
    "        len(timestamps) * n) * old_frame_interval / n + start\n",
    "    return new_frame_inds.astype(np.int64)\n",
    "\n",
    "dense_n = int(predict_stepsize / output_stepsize)\n",
    "output_timestamps = dense_timestamps(timestamps, dense_n) + 1\n",
    "frames = [\n",
    "    cv2.imread(frame_paths[timestamp - 1])\n",
    "    for timestamp in output_timestamps\n",
    "]\n",
    "\n",
    "pose_datasample = [\n",
    "    pose_datasample[timestamp - 1] for timestamp in output_timestamps\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49614666-c51b-45a1-9658-2d2192a15a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_frames = visualize(pose_config, frames, stdet_results, pose_datasample,\n",
    "                       None)\n",
    "vid = mpy.ImageSequenceClip(vis_frames, fps=output_fps)\n",
    "vid.write_videofile(out_filename)\n",
    "tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b009e-2616-401c-814e-326c99aaca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_extract(\n",
    "#     out_filename, out_dir=\"../extracted/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f05741-9b0b-4cc5-ad76-d7ac602caf76",
   "metadata": {},
   "source": [
    "# Add Label to Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce0f22-b7f4-48ee-a6c1-1a3aae2ac761",
   "metadata": {},
   "outputs": [],
   "source": [
    "anned_filename = 'data/skeleton/to-anno/30d_1s1_annotated.csv'\n",
    "pkl_final = 'data/skeleton/to-combine/30d_1s1.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6e203-1374-4c1b-823d-d1fb013761cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_label_map(file_path):\n",
    "    \"\"\"Load Label Map.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The file path of label map.\n",
    "\n",
    "    Returns:\n",
    "        dict: The label map (label name -> int).\n",
    "    \"\"\"\n",
    "    lines = open(file_path).readlines()\n",
    "    lines = [x.strip().split(': ') for x in lines]\n",
    "    return {x[1]: int(x[0]) for x in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a45a1-e630-4ad1-ac43-5fff8a27a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_label_map = load_label_map(label_map_stdet)\n",
    "\n",
    "stdet_label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ab6d2a-b959-4868-beab-f09b743e594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_annos = []\n",
    "\n",
    "with open(anned_filename, newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in spamreader:\n",
    "        if row[0] == 'none':\n",
    "            continue\n",
    "        label = stdet_label_map[row[0]]\n",
    "        id = float(row[1])\n",
    "        custom_annos.append([id, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8733dbe-bc1a-4550-9d38-eebfa19d8fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton_datasets = mmengine.load(pkl_filename)\n",
    "\n",
    "custom_dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af24374-c83a-432a-bc31-032e5bea093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, ann in enumerate(custom_annos):\n",
    "\n",
    "    fake_anno = dict(\n",
    "        frame_dir=osp.splitext(osp.basename(pkl_filename))[0]+\"_\"+str(ann[0]))\n",
    "\n",
    "    for j, data in enumerate(skeleton_datasets):\n",
    "        if fake_anno['frame_dir'] == data['frame_dir']:\n",
    "            fake_anno['frame_dir'] += '_' + str(index)\n",
    "            fake_anno['label'] = ann[1]\n",
    "            fake_anno['img_shape'] = data['img_shape']\n",
    "            fake_anno['original_shape'] = data['original_shape']\n",
    "            fake_anno['num_clips'] = 1\n",
    "            fake_anno['total_frames'] = data['total_frames']\n",
    "            fake_anno['clip_len'] = data['total_frames']\n",
    "            fake_anno['keypoint'] = data['keypoint']\n",
    "            fake_anno['keypoint_score'] = data['keypoint_score']\n",
    "\n",
    "    custom_dataset.append(fake_anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b5d5c-6606-44e8-ba9a-07bea8dd4966",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmengine.dump(custom_dataset, pkl_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b61527-d5be-43c8-b0f7-1cb40ecb1372",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93574d8c-6665-4861-b2a3-96d339fb5c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def visualize(pose_config,\n",
    "              frames,\n",
    "              annotations,\n",
    "              pose_data_samples,\n",
    "              action_result,\n",
    "              plate=PLATEBLUE,\n",
    "              max_num=5):\n",
    "    \"\"\"Visualize frames with predicted annotations.\n",
    "\n",
    "    Args:\n",
    "        frames (list[np.ndarray]): Frames for visualization, note that\n",
    "            len(frames) % len(annotations) should be 0.\n",
    "        annotations (list[list[tuple]]): The predicted spatio-temporal\n",
    "            detection results.\n",
    "        pose_data_samples (list[list[PoseDataSample]): The pose results.\n",
    "        action_result (str): The predicted action recognition results.\n",
    "        pose_model (nn.Module): The constructed pose model.\n",
    "        plate (str): The plate used for visualization. Default: PLATEBLUE.\n",
    "        max_num (int): Max number of labels to visualize for a person box.\n",
    "            Default: 5.\n",
    "\n",
    "    Returns:\n",
    "        list[np.ndarray]: Visualized frames.\n",
    "    \"\"\"\n",
    "\n",
    "    assert max_num + 1 <= len(plate)\n",
    "    frames_ = cp.deepcopy(frames)\n",
    "    frames_ = [mmcv.imconvert(f, 'bgr', 'rgb') for f in frames_]\n",
    "    nf, na = len(frames), len(annotations)\n",
    "    assert nf % na == 0\n",
    "    nfpa = len(frames) // len(annotations)\n",
    "    anno = None\n",
    "    h, w, _ = frames[0].shape\n",
    "    scale_ratio = np.array([w, h, w, h])\n",
    "\n",
    "    # add pose results\n",
    "    if pose_data_samples:\n",
    "        pose_config = mmengine.Config.fromfile(pose_config)\n",
    "        visualizer = VISUALIZERS.build(pose_config.visualizer | {'line_width':5, 'bbox_color':(101,193,255), 'radius': 8})  # https://mmpose.readthedocs.io/en/latest/api.html#mmpose.visualization.PoseLocalVisualizer\n",
    "        visualizer.set_dataset_meta(pose_data_samples[0].dataset_meta)\n",
    "        for i, (d, f) in enumerate(zip(pose_data_samples, frames_)):\n",
    "            visualizer.add_datasample(\n",
    "                'result',\n",
    "                f,\n",
    "                data_sample=d,\n",
    "                draw_gt=False,\n",
    "                draw_heatmap=False,\n",
    "                draw_bbox=True,\n",
    "                draw_pred=True,\n",
    "                show=False,\n",
    "                wait_time=0,\n",
    "                out_file=None,\n",
    "                kpt_thr=0.3)\n",
    "            frames_[i] = visualizer.get_image()\n",
    "\n",
    "    for i in range(na):\n",
    "        anno = annotations[i]\n",
    "        if anno is None:\n",
    "            continue\n",
    "        for j in range(nfpa):\n",
    "            ind = i * nfpa + j\n",
    "            frame = frames_[ind]\n",
    "\n",
    "            # add spatio-temporal action detection results\n",
    "            for ann in anno:\n",
    "                box = ann[0]\n",
    "                label = ann[1]\n",
    "                if not len(label):\n",
    "                    continue\n",
    "                score = ann[2]\n",
    "                box = (box * scale_ratio).astype(np.int64)\n",
    "                st, ed = tuple(box[:2]), tuple(box[2:])\n",
    "                if not pose_data_samples:\n",
    "                    cv2.rectangle(frame, st, ed, plate[0], 2)\n",
    "\n",
    "                for k, lb in enumerate(label):\n",
    "                    if k >= max_num:\n",
    "                        break\n",
    "                    text = abbrev(lb)\n",
    "                    # text = ': '.join([text, f'{(score[k]*100):.1f}%'])  # to add score\n",
    "                    location = (0 + st[0], 18 + k * 18 + st[1])\n",
    "                    textsize = cv2.getTextSize(text, FONTFACE, FONTSCALE,\n",
    "                                               THICKNESS)[0]\n",
    "                    textwidth = textsize[0]\n",
    "                    diag0 = (location[0] + textwidth, location[1] - 14)\n",
    "                    diag1 = (location[0], location[1] + 2)\n",
    "                    cv2.rectangle(frame, diag0, diag1, plate[k + 1], -1)\n",
    "                    bahaya = ['melempar', 'membidik senapan', 'membidik pistol', 'memukul', 'menendang', 'menusuk']\n",
    "                    FONTCOLOR = (255, 0, 0) if lb in bahaya else (255, 255, 255)\n",
    "                    cv2.putText(frame, text, location, FONTFACE, FONTSCALE,\n",
    "                                FONTCOLOR, THICKNESS, LINETYPE)\n",
    "\n",
    "    return frames_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a133b9-29c7-48e8-8e43-9b2e1ece9ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_preds_gt = list([])\n",
    "\n",
    "# timestamps == ambil dari atas\n",
    "# stdet_preds == ambil dari atas\n",
    "\n",
    "for timestamp, stdet_pred in zip(timestamps, stdet_preds):\n",
    "    timestmp_anno = list([])\n",
    "    if stdet_pred != None:\n",
    "        for i, object in enumerate(stdet_pred):\n",
    "            object_anno = list([])\n",
    "            with open(anned_filename, newline='') as csvfile:\n",
    "                spamreader = csv.reader(csvfile, delimiter=',')\n",
    "                for row in spamreader:\n",
    "                    if row[0] == 'none':\n",
    "                        continue\n",
    "                    elif float(row[1]) == (float(timestamp) + (i+1)*0.001):\n",
    "                        object_anno.append(tuple([row[0], np.random.uniform(0.4, 1.0)]))\n",
    "            timestmp_anno.append(object_anno)\n",
    "    stdet_preds_gt.append(timestmp_anno)\n",
    "\n",
    "# stdet_preds = list[list[tuple]]\n",
    "# [\n",
    "#     [[(), ()],[()],[]],\n",
    "#     [[],[]]\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65084995-05ff-4699-8c6d-8e963a15c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdet_results = []\n",
    "\n",
    "# human_detections == ambil dari atas\n",
    "# new_h, new_w == ambil dari atas\n",
    "# predict_stepsize == ambil dari atas\n",
    "# output_stepsize == ambil dari atas\n",
    "# frame_paths == ambil dari atas\n",
    "# pose_datasample == ambil dari atas\n",
    "\n",
    "for timestamp, prediction in zip(timestamps, stdet_preds_gt):\n",
    "    human_detection = human_detections[timestamp - 1]\n",
    "    stdet_results.append(\n",
    "        pack_result(human_detection, prediction, h, w))\n",
    "\n",
    "def dense_timestamps(timestamps, n):\n",
    "    \"\"\"Make it nx frames.\"\"\"\n",
    "    old_frame_interval = (timestamps[1] - timestamps[0])\n",
    "    start = timestamps[0] - old_frame_interval / n * (n - 1) / 2\n",
    "    new_frame_inds = np.arange(\n",
    "        len(timestamps) * n) * old_frame_interval / n + start\n",
    "    return new_frame_inds.astype(np.int64)\n",
    "\n",
    "dense_n = int(predict_stepsize / output_stepsize)\n",
    "# output_timestamps = dense_timestamps(timestamps, dense_n)\n",
    "output_timestamps = dense_timestamps(timestamps, dense_n) + 1\n",
    "frames = [\n",
    "    cv2.imread(frame_paths[timestamp - 1])\n",
    "    # cv2.imread(\"../854x480-white-solid-color-background.jpg\")\n",
    "    for timestamp in output_timestamps\n",
    "]\n",
    "\n",
    "pose_datasample = [\n",
    "    pose_datasample[timestamp - 1] for timestamp in output_timestamps\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249156d8-9abb-430d-8660-ffa431cc0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose_config == ambil dari atas\n",
    "# frames == ambil dari atas\n",
    "\n",
    "# pose_datasample == ambil dari atas\n",
    "\n",
    "vis_frames = visualize(pose_config, frames, stdet_results, pose_datasample,\n",
    "                       None)\n",
    "vid = mpy.ImageSequenceClip(vis_frames, fps=output_fps)\n",
    "vid.write_videofile(out_filename)\n",
    "tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe022e-78e3-4c08-bc71-7ba293e895c5",
   "metadata": {},
   "source": [
    "# Combine PKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3415cdce-806f-4f55-bcbe-fca63a12cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf48c26-7996-41fb-b6b9-ef1c7954427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickles_path = 'data/skeleton/to-combine'\n",
    "split_ratio = 0.5\n",
    "combined_pkl = 'data/skeleton/ciis_' + (str(split_ratio)).replace(\".\", \"s\") + '_v1.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3f6b6f7-20c9-4b6f-ab2e-b12520f58599",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_datasets = dict(split=dict(xsub_train=[],\n",
    "                                  xsub_val=[],\n",
    "                                  xview_train=[],\n",
    "                                  xview_val=[]),\n",
    "                       annotations=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2cb2274-76a3-4ce1-84fe-59684f6b91c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70d_1s2.pkl\n",
      "50d_1s1.pkl\n",
      "70d_1s3.pkl\n",
      "50d_1s3.pkl\n",
      "30d_1s3.pkl\n",
      "50d_1s2.pkl\n",
      "30d_1s2.pkl\n",
      "30d_1s1.pkl\n",
      "70d_1s1.pkl\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(pickles_path):\n",
    "    print(file)\n",
    "    if not file.endswith('.pkl'):\n",
    "        continue\n",
    "    custom_dataset = mmengine.load(os.path.join(pickles_path, file))\n",
    "    for i, data in enumerate(custom_dataset):\n",
    "        custom_datasets['annotations'].append(data)\n",
    "        if (i % 10) < (split_ratio * 10):\n",
    "            custom_datasets['split']['xsub_train'].append(data['frame_dir'])\n",
    "            custom_datasets['split']['xview_train'].append(data['frame_dir'])\n",
    "        else:\n",
    "            custom_datasets['split']['xsub_val'].append(data['frame_dir'])\n",
    "            custom_datasets['split']['xview_val'].append(data['frame_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec9f3f55-ec49-4110-a576-49bdced5d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmengine.dump(custom_datasets, combined_pkl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
